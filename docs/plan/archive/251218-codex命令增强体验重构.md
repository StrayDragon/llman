当前我们的codex的实现有点问题, 不能很好的管理用户的账号, 请你根据codex官方提供的文档, 重新设计codex账号切换体系

我们希望重新设计

在用户使用上仅有以下

```bash
llman x codex account -i # 交互模式: 提供对配置更新和删除(没有list)
llman x codex account  # 展示list 配置 脱敏
llman x codex run --group <profile-group> -- <args> # 根据上一次选中的配置转发到目标codex函数中
llman x codex # 进入交互模式选择一个配置
```

提供的配置应该是最简化通用的不要提供好多配置用户不知道怎么配置, 就像 llman x cc 那样


# codex 官方配置文档

Codex should work out of the box for most users. But sometimes you want to configure Codex to your own liking to better suit your needs. For this there is a wide range of configuration options.

## Codex configuration file

The configuration file for Codex is located at `~/.codex/config.toml`.

To access the configuration file when you are using the Codex IDE extension, you can click the gear icon in the top right corner of the extension and then clicking `Codex Settings > Open config.toml`.

This configuration file is shared between the CLI and the IDE extension and can be used to configure things like the default model, [approval policies, sandbox settings](https://developers.openai.com/codex/security) or [MCP servers](https://developers.openai.com/codex/mcp) that Codex should have access to.

## High level configuration options

Codex provides a wide range of configuration options. Some of the most commonly changed settings are:

#### Default model

Pick which model Codex uses by default in both the CLI and IDE.

**Using `config.toml`:**

```toml
model = "gpt-5"
```

**Using CLI arguments:**

```shell
codex --model gpt-5
```

#### Model provider

Select the backend provider referenced by the active model. Be sure to [define the provider](https://github.com/openai/codex/blob/main/docs/config.md#model_providers) in your config first.

**Using `config.toml`:**

```toml
model_provider = "ollama"
```

**Using CLI arguments:**

```shell
codex --config model_provider="ollama"
```

#### Approval prompts

Control when Codex pauses to ask before running generated commands.

**Using `config.toml`:**

```toml
approval_policy = "on-request"
```

**Using CLI arguments:**

```shell
codex --ask-for-approval on-request
```

#### Sandbox level

Adjust how much filesystem and network access Codex has while executing commands.

**Using `config.toml`:**

```toml
sandbox_mode = "workspace-write"
```

**Using CLI arguments:**

```shell
codex --sandbox workspace-write
```

#### Reasoning depth

Tune how much reasoning effort the model applies when supported.

**Using `config.toml`:**

```toml
model_reasoning_effort = "high"
```

**Using CLI arguments:**

```shell
codex --config model_reasoning_effort="high"
```

#### Command environment

Restrict or expand which environment variables are forwarded to spawned commands.

**Using `config.toml`:**

```toml
[shell_environment_policy]

include_only = ["PATH", "HOME"]
```

**Using CLI arguments:**

```shell
codex --config shell_environment_policy.include_only='["PATH","HOME"]'
```

Profiles bundle a set of configuration values so you can jump between setups without editing `config.toml` each time. They currently apply to the Codex CLI.

Define profiles under `[profiles.<name>]` in `config.toml` and launch the CLI with `codex --profile <name>`:

```toml
model = "gpt-5-codex"

approval_policy = "on-request"

[profiles.deep-review]

model = "gpt-5-pro"

model_reasoning_effort = "high"

approval_policy = "never"

[profiles.lightweight]

model = "gpt-4.1"

approval_policy = "untrusted"
```

Running `codex --profile deep-review` will use the `gpt-5-pro` model with high reasoning effort and no approval policy. Running `codex --profile lightweight` will use the `gpt-4.1` model with untrusted approval policy. To make one profile the default, add `profile = "deep-review"` at the top level of `config.toml`; the CLI will load that profile unless you override it on the command line.

Values resolve in this order: explicit CLI flags (like `--model`) override everything, profile values come next, then root-level entries in `config.toml`, and finally the CLI’s built-in defaults. Use that precedence to layer common settings at the top level while letting each profile tweak just the fields that need to change.

## Feature flags

Optional and experimental capabilities are toggled via the `[features]` table in `config.toml`. If Codex emits a deprecation warning mentioning a legacy key (such as `experimental_use_exec_command_tool`), move that setting into `[features]` or launch the CLI with `codex --enable <feature>`.

```toml
[features]

streamable_shell = true          # enable the streamable exec tool

web_search_request = true        # allow the model to request web searches

# view_image_tool defaults to true; omit to keep defaults
```

### Supported features

| Key | Default | Stage | Description |
| --- | --- | --- | --- |
| `unified_exec` | false | Experimental | Use the unified PTY-backed exec tool |
| `streamable_shell` | false | Experimental | Use the streamable exec-command/write-stdin pair |
| `rmcp_client` | false | Experimental | Enable OAuth support for streamable HTTP MCP servers |
| `apply_patch_freeform` | false | Beta | Include the freeform `apply_patch` tool |
| `view_image_tool` | true | Stable | Include the `view_image` tool |
| `web_search_request` | false | Stable | Allow the model to issue web searches |
| `experimental_sandbox_command_assessment` | false | Experimental | Enable model-based sandbox risk assessment |
| `ghost_commit` | false | Experimental | Create a ghost commit each turn |
| `enable_experimental_windows_sandbox` | false | Experimental | Use the Windows restricted-token sandbox |

Omit feature keys to keep their defaults.
Legacy booleans such as `experimental_use_exec_command_tool`,`experimental_use_unified_exec_tool`, `include_apply_patch_tool`, and similar `experimental_use_*` entries are deprecated—migrate them to the matching `[features].<key>` flag to avoid repeated warnings.

### Enabling features quickly

- In `config.toml`: add `feature_name = true` under `[features]`.
- CLI onetime: `codex --enable feature_name`.
- Multiple flags: `codex --enable feature_a --enable feature_b`.
- Disable explicitly by setting the key to `false` in `config.toml`.

## Advanced configuration

### Custom model providers

Define additional providers and point `model_provider` at them:

```toml
model = "gpt-4o"

model_provider = "openai-chat-completions"

[model_providers.openai-chat-completions]

name = "OpenAI using Chat Completions"

base_url = "https://api.openai.com/v1"

env_key = "OPENAI_API_KEY"

wire_api = "chat"

query_params = {}

[model_providers.ollama]

name = "Ollama"

base_url = "http://localhost:11434/v1"

[model_providers.mistral]

name = "Mistral"

base_url = "https://api.mistral.ai/v1"

env_key = "MISTRAL_API_KEY"
```

Add request headers when needed:

```toml
[model_providers.example]

http_headers = { "X-Example-Header" = "example-value" }

env_http_headers = { "X-Example-Features" = "EXAMPLE_FEATURES" }
```

### Azure provider & per-provider tuning

```toml
[model_providers.azure]

name = "Azure"

base_url = "https://YOUR_PROJECT_NAME.openai.azure.com/openai"

env_key = "AZURE_OPENAI_API_KEY"

query_params = { api-version = "2025-04-01-preview" }

wire_api = "responses"

[model_providers.openai]

request_max_retries = 4

stream_max_retries = 10

stream_idle_timeout_ms = 300000
```

### Model reasoning, verbosity, and limits

```toml
model_reasoning_summary = "none"          # disable summaries

model_verbosity = "low"                   # shorten responses on Responses API providers

model_supports_reasoning_summaries = true # force reasoning on custom providers

model_context_window = 128000             # override when Codex doesn't know the window

model_max_output_tokens = 4096            # cap completion length
```

`model_verbosity` applies only to providers using the Responses API; Chat Completions providers will ignore the setting.

### Approval policies and sandbox modes

Pick approval strictness (affects when Codex pauses) and sandbox level (affects file/network access). See [Sandbox & approvals](https://developers.openai.com/codex/security) for deeper examples.

```toml
approval_policy = "untrusted"   # other options: on-request, on-failure, never

sandbox_mode = "workspace-write"

[sandbox_workspace_write]

exclude_tmpdir_env_var = false  # allow $TMPDIR

exclude_slash_tmp = false       # allow /tmp

writable_roots = ["/Users/YOU/.pyenv/shims"]

network_access = false          # opt in to outbound network
```

Disable sandboxing entirely (use only if your environment already isolates processes):

```toml
sandbox_mode = "danger-full-access"
```

### Rules (preview)

A `.rules` file lets you define fine-grained rules that govern Codex’s behavior, such as identifying commands that Codex is allowed to run *outside* the sandbox.

For example, suppose you created the file `~/.codex/rules/default.rules` with the following contents:

```python
# Rule that allows commands that start with \`gh pr view\` to run outside

# the sandbox for Codex's "shell tool."

prefix_rule(

    # The prefix to match.

    pattern = ["gh", "pr", "view"],

    # The action to take when Codex requests to run a matching command.

    decision = "allow",

    # \`match\` and \`not_match\` are optional "inline unit tests" where you can

    # provide examples of commands that should (or should not) match this rule,

    # respectively. The .rules file will fail to load if these tests fail.

    match = [

      "gh pr view 7888",

      "gh pr view --repo openai/codex",

      "gh pr view 7888 --json title,body,comments",

    ],

    not_match = [

      # Does not match because the \`pattern\` must be an exact prefix.

      "gh pr --repo openai/codex view 7888",

    ],

)
```

A `prefix_rule()` lets you pre-approve, prompt, or block commands before Codex runs them using the following options:

- `pattern` **(required)** is a non-empty list where each element is either a literal (e.g., `"pr"`) or a union of literals (e.g., `["view", "list"]`) that defines the *command prefix* to be matched by the rule. When Codex’s shell tool considers a command to run (which internally can be thought of as a list of arguments for [`execvp(3)`](https://linux.die.net/man/3/execvp)), it will compare the start of the list of arguments with those of the `pattern`.
	- Use a union to express alternatives for an individual argument. For example, `pattern = ["gh", "pr", ["view", "list"]]` would allow both `gh pr view` and `gh pr list` to run outside the sandbox.
- `decision` **(defaults to `"allow"`)** sets the strictness; Codex applies the most restrictive decision when multiple rules match (`forbidden` > `prompt` > `allow`)
	- `allow` means the command should be run automatically outside the sandbox: the user will not be consulted.
	- `prompt` means the user will be prompted to allow each individual invocation of a matching command. If approved, the command will be run outside the sandbox.
	- `forbidden` means the request will be rejected automatically without notifying the user.
- `match` and `not_match` **(defaults to `[]`)** act like tests that Codex validates when it loads your policy.

Codex loads every `*.rules` file under `~/.codex/rules` at startup; when you whitelist a command in the TUI, it appends a rule to `~/.codex/rules/default.rules` so future runs can skip the prompt.

Note the input language for a `.rules` file is [Starlark](https://github.com/bazelbuild/starlark/blob/master/spec.md). Its syntax is similar to Python’s, but it is designed to be a safe, embeddable language that can be interpeted without side-effects (such as touching the filesystem). Starlark’s affordances such as list comprehensions makes it possible to build up rules dynamically.

Finally, to test how a policy applies to a command without editing files, you can use the CLI helper:

```shell
$ codex execpolicy check --pretty --rules ~/.codex/rules/default.rules -- gh pr view 7888 --json title,body,comments

{

  "matchedRules": [

    {

      "prefixRuleMatch": {

        "matchedPrefix": [

          "gh",

          "pr",

          "view"

        ],

        "decision": "prompt"

      }

    }

  ],

  "decision": "prompt"

}
```

Pass multiple `--rules` flags to combine files and add `--pretty` for formatted JSON. The rules system is still in preview, so syntax and defaults may change.

### Shell environment templates

`shell_environment_policy` controls which environment variables Codex passes to any subprocess it launches (for example, when running a tool-command the model proposes). Start from a clean slate (`inherit = "none"`) or a trimmed set (`inherit = "core"`), then layer on excludes, includes, and overrides to avoid leaking secrets while still providing the paths, keys, or flags your tasks need.

```toml
[shell_environment_policy]

inherit = "none"

set = { PATH = "/usr/bin", MY_FLAG = "1" }

ignore_default_excludes = false

exclude = ["AWS_*", "AZURE_*"]

include_only = ["PATH", "HOME"]
```

Patterns are case-insensitive globs (`*`, `?`, `[A-Z]`); `ignore_default_excludes = false` keeps the automatic KEY/SECRET/TOKEN filter before your includes/excludes run.

### MCP servers

See the dedicated [MCP guide](https://developers.openai.com/codex/mcp) for full server setups and toggle descriptions. Below is a minimal STDIO example using the Context7 MCP server:

```toml
[mcp_servers.context7]

command = "npx"

args = ["-y", "@upstash/context7-mcp"]
```

### Observibility and telemetry

Enable OpenTelemetry (Otel) log export to track Codex runs (API requests, SSE/events, prompts, tool approvals/results). Disabled by default; opt in via `[otel]`:

```toml
[otel]

environment = "staging"   # defaults to "dev"

exporter = "none"         # set to otlp-http or otlp-grpc to send events

log_user_prompt = false   # redact user prompts unless explicitly enabled
```

Choose an exporter:

```toml
[otel]

exporter = { otlp-http = {

  endpoint = "https://otel.example.com/v1/logs",

  protocol = "binary",

  headers = { "x-otlp-api-key" = "${OTLP_TOKEN}" }

}}
```
```toml
[otel]

exporter = { otlp-grpc = {

  endpoint = "https://otel.example.com:4317",

  headers = { "x-otlp-meta" = "abc123" }

}}
```

If `exporter = "none"` Codex records events but sends nothing. Exporters batch asynchronously and flush on shutdown. Event metadata includes service name, CLI version, env tag, conversation id, model, sandbox/approval settings, and per-event fields (see Config reference table below).

### Notifications

Use `notify` to trigger an external program whenever Codex emits supported events (today: `agent-turn-complete`). This is handy for desktop toasts, chat webhooks, CI updates, or any side-channel alerting that the built-in TUI notifications don’t cover.

```toml
notify = ["python3", "/path/to/notify.py"]
```

Example `notify.py` (truncated) that reacts to `agent-turn-complete`:

```python
#!/usr/bin/env python3

import json, subprocess, sys

def main() -> int:

    notification = json.loads(sys.argv[1])

    if notification.get("type") != "agent-turn-complete":

        return 0

    title = f"Codex: {notification.get('last-assistant-message', 'Turn Complete!')}"

    message = " ".join(notification.get("input-messages", []))

    subprocess.check_output([

        "terminal-notifier",

        "-title", title,

        "-message", message,

        "-group", "codex-" + notification.get("thread-id", ""),

        "-activate", "com.googlecode.iterm2",

    ])

    return 0

if __name__ == "__main__":

    sys.exit(main())
```

Place the script somewhere on disk and point `notify` to it. For lighter in-terminal alerts, toggle `tui.notifications` instead.

## Personalizing the Codex IDE Extension

Additionally to configuring the underlying Codex agent through your `config.toml` file, you can also configure the way you use the Codex IDE extension.

To see the list of available configuration options, click the gear icon in the top right corner of the extension and then click `IDE settings`.

To define your own keyboard shortcuts to trigger Codex or add something to the Codex context, you can click the gear icon in the top right corner of the extension and then click `Keyboard shortcuts`.

## Configuration options

Key

`model`

Type / Values

`string`

Details

Model to use (e.g., \`gpt-5-codex\`).

Key

`model_provider`

Type / Values

`string`

Details

Provider id from \`model\_providers\` (default: \`openai\`).

Key

`model_context_window`

Type / Values

`number`

Details

Context window tokens available to the active model.

Key

`model_max_output_tokens`

Type / Values

`number`

Details

Maximum number of tokens Codex may request from the model.

Key

`approval_policy`

Type / Values

`untrusted | on-failure | on-request | never`

Details

Controls when Codex pauses for approval before executing commands.

Key

`sandbox_mode`

Type / Values

`read-only | workspace-write | danger-full-access`

Details

Sandbox policy for filesystem and network access during command execution.

Key

`sandbox_workspace_write.writable_roots`

Type / Values

`array<string>`

Details

Additional writable roots when \`sandbox\_mode = "workspace-write"\`.

Key

`sandbox_workspace_write.network_access`

Type / Values

`boolean`

Details

Allow outbound network access inside the workspace-write sandbox.

Key

`sandbox_workspace_write.exclude_tmpdir_env_var`

Type / Values

`boolean`

Details

Exclude \`$TMPDIR\` from writable roots in workspace-write mode.

Key

`sandbox_workspace_write.exclude_slash_tmp`

Type / Values

`boolean`

Details

Exclude \`/tmp\` from writable roots in workspace-write mode.

Key

`notify`

Type / Values

`array<string>`

Details

Command invoked for notifications; receives a JSON payload from Codex.

Key

`instructions`

Type / Values

`string`

Details

Reserved for future use; prefer \`experimental\_instructions\_file\` or \`AGENTS.md\`.

Key

`mcp_servers.<id>.command`

Type / Values

`string`

Details

Launcher command for an MCP stdio server.

Key

`mcp_servers.<id>.args`

Type / Values

`array<string>`

Details

Arguments passed to the MCP stdio server command.

Key

`mcp_servers.<id>.env`

Type / Values

`map<string,string>`

Details

Environment variables forwarded to the MCP stdio server.

Key

`mcp_servers.<id>.env_vars`

Type / Values

`array<string>`

Details

Additional environment variables to whitelist for an MCP stdio server.

Key

`mcp_servers.<id>.cwd`

Type / Values

`string`

Details

Working directory for the MCP stdio server process.

Key

`mcp_servers.<id>.url`

Type / Values

`string`

Details

Endpoint for an MCP streamable HTTP server.

Key

`mcp_servers.<id>.bearer_token_env_var`

Type / Values

`string`

Details

Environment variable sourcing the bearer token for an MCP HTTP server.

Key

`mcp_servers.<id>.enabled`

Type / Values

`boolean`

Details

Disable an MCP server without removing its configuration.

Key

`mcp_servers.<id>.startup_timeout_sec`

Type / Values

`number`

Details

Override the default 10s startup timeout for an MCP server.

Key

`mcp_servers.<id>.tool_timeout_sec`

Type / Values

`number`

Details

Override the default 60s per-tool timeout for an MCP server.

Key

`mcp_servers.<id>.enabled_tools`

Type / Values

`array<string>`

Details

Allow list of tool names exposed by the MCP server.

Key

`mcp_servers.<id>.disabled_tools`

Type / Values

`array<string>`

Details

Deny list applied after \`enabled\_tools\` for the MCP server.

Key

`features.unified_exec`

Type / Values

`boolean`

Details

Use the unified PTY-backed exec tool (experimental).

Key

`features.streamable_shell`

Type / Values

`boolean`

Details

Switch to the streamable exec command/write-stdin tool pair (experimental).

Key

`features.rmcp_client`

Type / Values

`boolean`

Details

Enable the Rust MCP client to unlock OAuth for HTTP servers (experimental).

Key

`features.apply_patch_freeform`

Type / Values

`boolean`

Details

Expose the freeform \`apply\_patch\` tool (beta).

Key

`features.view_image_tool`

Type / Values

`boolean`

Details

Allow Codex to attach local images via the \`view\_image\` tool (stable; on by default).

Key

`features.web_search_request`

Type / Values

`boolean`

Details

Allow the model to issue web searches (stable).

Key

`features.experimental_sandbox_command_assessment`

Type / Values

`boolean`

Details

Enable model-based sandbox risk assessment (experimental).

Key

`features.ghost_commit`

Type / Values

`boolean`

Details

Create a ghost commit on each turn (experimental).

Key

`features.enable_experimental_windows_sandbox`

Type / Values

`boolean`

Details

Run the Windows restricted-token sandbox (experimental).

Key

`experimental_use_rmcp_client`

Type / Values

`boolean`

Details

Deprecated; replace with \`\[features\].rmcp\_client\` or \`codex --enable rmcp\_client\`.

Key

`model_providers.<id>.name`

Type / Values

`string`

Details

Display name for a custom model provider.

Key

`model_providers.<id>.base_url`

Type / Values

`string`

Details

API base URL for the model provider.

Key

`model_providers.<id>.env_key`

Type / Values

`string`

Details

Environment variable supplying the provider API key.

Key

`model_providers.<id>.query_params`

Type / Values

`map<string,string>`

Details

Extra query parameters appended to provider requests.

Key

`model_providers.<id>.request_max_retries`

Type / Values

`number`

Details

Retry count for HTTP requests to the provider (default: 4).

Key

`model_providers.<id>.stream_max_retries`

Type / Values

`number`

Details

Retry count for SSE streaming interruptions (default: 5).

Key

`model_providers.<id>.stream_idle_timeout_ms`

Type / Values

`number`

Details

Idle timeout for SSE streams in milliseconds (default: 300000).

Key

`model_reasoning_summary`

Type / Values

`auto | concise | detailed | none`

Details

Select reasoning summary detail or disable summaries entirely.

Key

`model_supports_reasoning_summaries`

Type / Values

`boolean`

Details

Force Codex to send reasoning metadata even for unknown models.

Key

`model_reasoning_summary_format`

Type / Values

`none | experimental`

Details

Override the format of reasoning summaries (experimental).

Key

`shell_environment_policy.inherit`

Type / Values

`all | core | none`

Details

Baseline environment inheritance when spawning subprocesses.

Key

`shell_environment_policy.ignore_default_excludes`

Type / Values

`boolean`

Details

Keep variables containing KEY/SECRET/TOKEN before other filters run.

Key

`shell_environment_policy.exclude`

Type / Values

`array<string>`

Details

Glob patterns for removing environment variables after the defaults.

Key

`shell_environment_policy.include_only`

Type / Values

`array<string>`

Details

Whitelist of patterns; when set only matching variables are kept.

Key

`shell_environment_policy.set`

Type / Values

`map<string,string>`

Details

Explicit environment overrides injected into every subprocess.

Key

`project_doc_max_bytes`

Type / Values

`number`

Details

Maximum bytes read from \`AGENTS.md\` when building project instructions.

Key

`project_doc_fallback_filenames`

Type / Values

`array<string>`

Details

Additional filenames to try when \`AGENTS.md\` is missing.

Key

`profile`

Type / Values

`string`

Details

Default profile applied at startup (equivalent to \`--profile\`).

Key

`profiles.<name>.*`

Type / Values

`various`

Details

Profile-scoped overrides for any of the supported configuration keys.

Key

`history.persistence`

Type / Values

`save-all | none`

Details

Control whether Codex saves session transcripts to history.jsonl.

Key

`history.max_bytes`

Type / Values

`number`

Details

Reserved for future use; currently not enforced.

Key

`file_opener`

Type / Values

`vscode | vscode-insiders | windsurf | cursor | none`

Details

URI scheme used to open citations from Codex output (default: \`vscode\`).

Key

`otel.environment`

Type / Values

`string`

Details

Environment tag applied to emitted OpenTelemetry events (default: \`dev\`).

Key

`otel.exporter`

Type / Values

`none | otlp-http | otlp-grpc`

Details

Select the OpenTelemetry exporter and provide any endpoint metadata.

Key

`otel.log_user_prompt`

Type / Values

`boolean`

Details

Opt in to exporting raw user prompts with OpenTelemetry logs.

Key

`tui`

Type / Values

`table`

Details

TUI-specific options such as enabling inline desktop notifications.

Key

`tui.notifications`

Type / Values

`boolean | array<string>`

Details

Enable TUI notifications; optionally restrict to specific event types.

Key

`hide_agent_reasoning`

Type / Values

`boolean`

Details

Suppress reasoning events in both the TUI and \`codex exec\` output.

Key

`show_raw_agent_reasoning`

Type / Values

`boolean`

Details

Surface raw reasoning content when the active model emits it.

Key

`experimental_instructions_file`

Type / Values

`string (path)`

Details

Experimental replacement for built-in instructions instead of \`AGENTS.md\`.

Key

`experimental_use_exec_command_tool`

Type / Values

`boolean`

Details

Deprecated; use \`\[features\].unified\_exec\` or \`codex --enable unified\_exec\`.

Key

`projects.<path>.trust_level`

Type / Values

`string`

Details

Mark a project or worktree as trusted (only \`"trusted"\` is recognized).

Key

`tools.web_search`

Type / Values

`boolean`

Details

Deprecated; use \`\[features\].web\_search\_request\` or \`codex --enable web\_search\_request\`.

Key

`tools.view_image`

Type / Values

`boolean`

Details

Deprecated; use \`\[features\].view\_image\_tool\` or \`codex --enable view\_image\_tool\`.

# 几个参考配置(一般都是针对某个供应商特化, 记住我们的目的是自由切换

## minimax

在 Codex CLI 中配置 MiniMax API
重要提示：使用前请先清除 OpenAI 环境变量在配置前，请确保清除以下 OpenAI 相关的环境变量，以免影响 MiniMax API 的正常使用：

    OPENAI_API_KEY
    OPENAI_BASE_URL

    编辑 Codex 的配置文件，路径为 .codex/config.toml，将以下配置添加到配置文件中。

    base_url 需根据地理位置设置：国内用户使用 https://api.minimaxi.com/v1，国际用户使用 https://api.minimax.io/v1

[model_providers.minimax]
name = "MiniMax Chat Completions API"
base_url = "https://api.minimaxi.com/v1"
env_key = "MINIMAX_API_KEY"
wire_api = "chat"
requires_openai_auth = false
request_max_retries = 4
stream_max_retries = 10
stream_idle_timeout_ms = 300000

[profiles.m2]
model = "codex-MiniMax-M2"
model_provider = "minimax"

    出于安全考虑，请在当前终端会话中通过环境变量设置 API Key，其中，需要将 MINIMAX_API_KEY 替换为从 MiniMax 开发者平台 (国际用户可访问 MiniMax Developer Platform) 获取的 API Key

export MINIMAX_API_KEY="<MINIMAX_API_KEY>"

    使用指定的配置文件启动 Codex CLI。

codex --profile m2


## rightcode

Codex安装教程

    使用npm安装codex

npm install -g @openai/codex

    创建(如果不存在)或编辑 ~/.codex/config.toml

model_provider = "right"
model = "gpt-5.1-codex"
model_reasoning_effort = "high"
disable_response_storage = true


[model_providers.right]
name = "right"
base_url = "https://right.codes/codex/v1"
wire_api = "responses"
requires_openai_auth = true

    创建(如果不存在)或编辑 ~/.codex/auth.json

{
  "OPENAI_API_KEY": "你的API密钥"
}
